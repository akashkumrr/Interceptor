{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrap Company jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import traceback\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from datetime import date, datetime, timedelta\n",
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs are saved as list of dictionaries eg. [{job1 details},{job2 details},...]\n",
    "# SAVED JOBS data on DISK\n",
    "\n",
    "def load_saved_jobs():\n",
    "    try:\n",
    "        with open('saved_jobs_disk.pickle', 'rb') as handle:\n",
    "            saved_jobs_list = pickle.load(handle)        \n",
    "            print(str(len(saved_jobs_list))+\" saved jobs loaded!!!\")\n",
    "    except Exception as e:\n",
    "        print(\"No saved jobs exists on disk...\")\n",
    "        saved_jobs_list=[]\n",
    "    return saved_jobs_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_job(job_dict):\n",
    "    print(job_dict[\"Company_name\"]+' | Experience( Years ) : '+str(job_dict['Experience'])+ \" | LinkedIn Job Id : \"+job_dict['Job_id'] + \" | Date Posted \"+job_dict[\"Time_posted\"])\n",
    "        #print(b[idx]['Job_id'])\n",
    "    print(job_dict[\"Job_position\"])\n",
    "    print(job_dict[\"Job_location\"])\n",
    "    if(job_dict['Job_apply_link']!=\"NA\"):\n",
    "        print(job_dict['Job_apply_link']+\"\\n\\n\")\n",
    "    else:\n",
    "        print(\"https://www.linkedin.com/jobs/view/\"+job_dict['Job_id']+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_date():\n",
    "    curr_date = strftime(\"%Y-%m-%d\", gmtime())\n",
    "    convert_date = datetime.strptime(curr_date, '%Y-%m-%d')\n",
    "    days_to_subtract = 14\n",
    "    two_weeks_before_date = convert_date - timedelta(days=days_to_subtract)\n",
    "    two_weeks_before_date = two_weeks_before_date.strftime(\"%Y-%m-%d\")\n",
    "    return two_weeks_before_date\n",
    "\n",
    "def get_digit_from_time_posted(sentence):    \n",
    "    trim = re.compile(r'[^\\d.]+')   \n",
    "    result = trim.sub('', sentence)\n",
    "    result2 = re.findall(\"\\d+\",result)[0]\n",
    "    return int(result2)\n",
    "\n",
    "def days_minus(sentence):\n",
    "    time_posted_number = get_digit_from_time_posted(sentence)\n",
    "    if 'week' in sentence:\n",
    "        days_to_subtract = 7*time_posted_number\n",
    "    if 'month' in sentence:\n",
    "        days_to_subtract = 30*time_posted_number\n",
    "    if 'day' in sentence:\n",
    "        days_to_subtract = 1*time_posted_number\n",
    "    if 'hour' in sentence:\n",
    "        days_to_subtract = 0\n",
    "\n",
    "    return int(days_to_subtract)\n",
    "\n",
    "def calculate_date(sentence):\n",
    "    curr_date = strftime(\"%Y-%m-%d\", gmtime())\n",
    "    convert_date = datetime.strptime(curr_date, '%Y-%m-%d')\n",
    "    subtract_days = days_minus(sentence)\n",
    "    date_posted = convert_date - timedelta(days=subtract_days)\n",
    "    date_posted = date_posted.strftime(\"%Y-%m-%d\")\n",
    "    return date_posted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To save the parsed data in format of a dict and append it into stored_jobs_list\n",
    "# this function receives a single job data \n",
    "def make_dictionary(jid,gid,small_desc,big_desc,job_link,cid,date_added):\n",
    "    global saved_jobs_list\n",
    "    small_desc = small_desc.splitlines()\n",
    "\n",
    "    job_dict = {} \n",
    "    job_dict[\"Job_id\"] = jid    \n",
    "    job_dict[\"Company_name\"] = small_desc[1]\n",
    "    job_dict[\"Job_position\"] = small_desc[0]\n",
    "    job_dict[\"Job_location\"] = small_desc[2]\n",
    "\n",
    "\n",
    "    ps = nltk.PorterStemmer()\n",
    "\n",
    "    # Time posted is current date by default if not mentioned\n",
    "    job_dict[\"Time_posted\"] = get_default_date()\n",
    "\n",
    "    date_added = date_added.replace('\\n','')\n",
    "    stemmed_op=[]\n",
    "    stemmed_op.append(\" \".join([ps.stem(i) for i in date_added.split()]))\n",
    "\n",
    "    job_dict[\"Time_posted\"] = calculate_date(stemmed_op[0])\n",
    "\n",
    "    # get description and lINK from Big desc by opening\n",
    "    job_dict[\"job_description\"] = big_desc\n",
    "    job_dict[\"Job_apply_link\"] = job_link\n",
    "    job_dict[\"Geo_id\"] = gid\n",
    "    job_dict[\"Company_id\"] = cid\n",
    "    \n",
    "    if(job_dict[\"Job_apply_link\"]==\"NA\"):\n",
    "        job_dict[\"Job_apply_link\"]=\"https://www.linkedin.com/jobs/view/\"+job_dict['Job_id']\n",
    "    \n",
    "    # NOW saving jobs will be done in experience funtion\n",
    "    ###saved_jobs_list.append(job_dict)\n",
    "    \n",
    "    if(\"job_description\" in job_dict):\n",
    "        count_exp_mentions = 0\n",
    "        desc=job_dict['job_description']\n",
    "\n",
    "        max_year = 0\n",
    "        is_experience_req = False\n",
    "\n",
    "        # Check 1\n",
    "        ####!! This method is paragraph work experience\n",
    "        q = desc # \"job_description 100  \\n Exp: \\n 0-3 years; Job Code\"\n",
    "        q= (re.sub(r'(\\\\n+)', r'#', desc))\n",
    "        res = re.compile(r'#(yrs|years|year|exp|experience).*(yrs|years|year)#').search(q.lower())\n",
    "\n",
    "        # if paragraph type experience listing is there, then extract maximum experience year \n",
    "        if res is not None:\n",
    "            res=res.group(0)\n",
    "            num_list=[float(num) for num in re.findall(r'\\d*\\.\\d+|\\d+', res)]\n",
    "\n",
    "            is_experience_req =True\n",
    "            max_year=max(num_list)\n",
    "\n",
    "        # Check 2\n",
    "        # This method is for detecting work experience in same line\n",
    "        desc_format = desc.replace(\"\\\\n\", \"\\n\")     \n",
    "        lined_desc = re.split(\"\\n|\\,|\\.|\\;\",desc_format)\n",
    "\n",
    "        # removing index and formating dataframe to be ready for analysis\n",
    "        max_year_exp_list =[0]\n",
    "\n",
    "        for i,line in enumerate(lined_desc):\n",
    "            if('experience' in line.lower()  or 'expertise' in line.lower() ):\n",
    "                count_exp_mentions+=1            \n",
    "\n",
    "            if(('year' in line.lower() or 'yrs' in line.lower()) and ('experience' in line.lower() or 'exp' in line.lower()) ):\n",
    "\n",
    "                # finding all the numbers in the string\n",
    "                num_list=[float(num) for num in re.findall(r'\\d*\\.\\d+|\\d+', line)]\n",
    "                if num_list:\n",
    "                    max_year_exp_list.append(max(num_list))\n",
    "                is_experience_req = True\n",
    "\n",
    "        if(max_year<max(max_year_exp_list)):\n",
    "            max_year=max(max_year_exp_list)\n",
    "\n",
    "\n",
    "        # override Experience\n",
    "        job_dict[\"Experience\"] = max_year\n",
    "        \n",
    "        # UPDATE SAVED JOBS LIST\n",
    "        saved_jobs_list.append(job_dict)\n",
    "        \n",
    "        # SAVE IN PICKLE\n",
    "        with open('saved_jobs_disk.pickle', 'rb') as handle:\n",
    "            loaded_jobs = pickle.load(handle)\n",
    "\n",
    "\n",
    "        # if jobs are scrapped i.e. added in our global saved_jobs_list, then dump it in into pickle\n",
    "        if(len(loaded_jobs)<=len(saved_jobs_list)):\n",
    "            with open('saved_jobs_disk.pickle', 'wb') as handle:\n",
    "                pickle.dump(saved_jobs_list, handle)\n",
    "                print(\"Successfully saved : \"+str(len(saved_jobs_list))+ \" jobs on disk!\")\n",
    "        else:\n",
    "            print(\"No jobs were scrapped!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2454502607'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_link_for_job_id(link):\n",
    "    Jid = re.compile(r'/jobs/view/\\d+').search(link).group(0)\n",
    "    Jid = Jid.split('/jobs/view/')[1]\n",
    "    return Jid\n",
    "\n",
    "q2 = \"/jobs/view/2454502607/?eBP=CwEAAAF4gclyuiceBUejrW7FlNeHQ7rw-6nCXbBJFE0TFnIVFaKKrOKSjCvgCJgcHCOIr6q-kNYrMKsBFKoEq8Ymc8QFranG7m_FLbHaHg79gbov_pRrHVaFFQq9WEO7FkV99UnZDzeNvJxIKll9PR3QFWOVNokEvik_S4vIzfnHzzNbmhYLkVadMg8xjHtUbu1yzr5EmrJQSa52I_Z0-2DgOkZRlqRPP29cWFhuhqkBsDO_jYuSx5gMOfbuLU-Z_t3JG1cj5-X5fu2MdNh69rK08ste-rJFQv49Lf4rGJdtK2cdL4lyqypxPKfNjYF-s287g9PqdVk1kOuSNR9A6mDyIVk_LyV-knI5&recommendedFlavor=SCHOOL_RECRUIT&refId=dhBqegnY4ua7D4VfqZ6ixQ%3D%3D&trackingId=K7w1GgyIsTYVghj28UiYKQ%3D%3D&trk=flagship3_search_srp_jobs\"\n",
    "\n",
    "trim_link_for_job_id(q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1426'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_link_for_company_id(link):\n",
    "    Jid = re.compile(r'/company/\\d+').search(link).group(0)\n",
    "    Jid = Jid.split('/company/')[1]\n",
    "    return Jid\n",
    "\n",
    "q3 = \"/company/1426/\"\n",
    "\n",
    "trim_link_for_company_id(q3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Work experience','years','must have','Seniority Level','Entry level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium Code starts here :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '']\n",
      " url and previous session loaded!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_opened_url_and_opened_session():\n",
    "    try:\n",
    "        with open('url_session_stored.pickle', 'rb') as handle:\n",
    "            pickled_url_session_list = pickle.load(handle)  \n",
    "            opened_url = pickled_url_session_list[0]\n",
    "            opened_session_id = pickled_url_session_list[1]\n",
    "            \n",
    "            print(pickled_url_session_list)\n",
    "            print(\" url and previous session loaded!!!\")\n",
    "    except Exception as e:\n",
    "        print(\"No existing url or session exisits. \"+str(e))\n",
    "        opened_url=''\n",
    "        opened_session_id=''\n",
    "        \n",
    "        pickled_url_session_list=[]\n",
    "        pickled_url_session_list=[opened_url,opened_session_id]\n",
    "        \n",
    "    return pickled_url_session_list\n",
    "        \n",
    "load_opened_url_and_opened_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '']\n",
      " url and previous session loaded!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_urlsession_list= load_opened_url_and_opened_session()\n",
    "loaded_urlsession_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opened_url_and_opened_session(pickled_url_session_list):\n",
    "    try:\n",
    "        with open('url_session_stored.pickle', 'wb') as handle:\n",
    "            pickle.dump(pickled_url_session_list, handle)\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION OCCURED!!## \"+str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_opened_url_and_opened_session(['',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jobs are saved as list of dictionaries eg. [{job1 details},{job2 details},...]\n",
    "\n",
    "# when running from cmd prompt, this function can't be run from using previous session as the command line closes the \n",
    "# scoket connection when the script is stopped. \n",
    "# So, run it using jupyter notebook which doesn't closes the connection.\n",
    "def scrap_jobs(the_company_id,role='software'):\n",
    "    \n",
    "    global saved_jobs_list\n",
    "\n",
    "    options = Options()\n",
    "    #options.headless = True\n",
    "\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    # for Turning of warnings in console selenium INFO:CONSOLE off\n",
    "\n",
    "    capa = DesiredCapabilities.CHROME\n",
    "    capa[\"pageLoadStrategy\"] = \"none\"\n",
    "    # This will now not wait for any page to load\n",
    "    # Whether to open a new session or use already exsisting session\n",
    "    # it saves us from logging in everytime\n",
    "\n",
    "    loaded_urlsession_list = load_opened_url_and_opened_session()\n",
    "    opened_url = loaded_urlsession_list[0]\n",
    "    opened_session_id = loaded_urlsession_list[1]\n",
    "\n",
    "    try:\n",
    "        if(opened_url==''):\n",
    "            print(\"No existing session found! Opening new session...\")\n",
    "            driver = webdriver.Chrome(options=options,desired_capabilities=capa)\n",
    "            driver.get(\"https://www.linkedin.com/jobs/\")\n",
    "\n",
    "            # Login\n",
    "            WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, 'session_key')))\n",
    "            time.sleep(2)\n",
    "            elem = driver.find_element_by_name('session_key')\n",
    "            elem.send_keys(\"your_email_address\")\n",
    "\n",
    "            WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.NAME, 'session_password')))\n",
    "            elem = driver.find_element_by_name('session_password')\n",
    "            elem.send_keys(\"your_password\")\n",
    "\n",
    "            elem = driver.find_element_by_class_name('sign-in-form__submit-btn')\n",
    "            elem.click()\n",
    "            #Login Complete\n",
    "        else:\n",
    "            print(\"Using previous session.\")\n",
    "            driver = webdriver.Remote(command_executor=opened_url,options=options,desired_capabilities=capa)\n",
    "            driver.close()   # this prevents the dummy browser\n",
    "            driver.session_id = opened_session_id\n",
    "            \n",
    "            print(type(driver.session_id))\n",
    "\n",
    "            #driver.get(\"https://www.linkedin.com/jobs/\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured... Couldn't connect to existing session. probably cuz session was closed... Open new window\"+str(e))\n",
    "\n",
    "        # resetting stored session id and url, Have to open new session\n",
    "        print(traceback.format_exc())\n",
    "        pickled_url_session_list=[]\n",
    "        pickled_url_session_list=[opened_url,opened_session_id]\n",
    "        save_opened_url_and_opened_session(pickled_url_session_list)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    actions = ActionChains(driver)\n",
    "    start = time.time()\n",
    "    print('Bot gearing up...')\n",
    "\n",
    "\n",
    "\n",
    "    time.sleep(2) # very important for making wait to login\n",
    "\n",
    "    ## TO be passed as arguments in this function\n",
    "    # the_company_id='1028%2C10173544%2C5208%2C10171627%2C2722798%2C976272'\n",
    "    # role='developer'\n",
    "\n",
    "    job_page=1;\n",
    "    skip_jobs_parsed_till_now = (job_page-1)*25\n",
    "    \n",
    "    try:\n",
    "\n",
    "        linkedin_link_for_job_search = 'https://www.linkedin.com/jobs/search/?f_C='+str(the_company_id)+'&geoId=92000000&keywords='+role+'&start='+str(skip_jobs_parsed_till_now)\n",
    "        driver.get(linkedin_link_for_job_search)\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured... Couldn't connect to existing session. probably cuz session was closed... Open new window\"+str(e))\n",
    "    \n",
    "        # resetting stored session id and url, Have to open new session\n",
    "        print(traceback.format_exc())       \n",
    "        \n",
    "        # resetting the URL and SESSION ids\n",
    "        save_opened_url_and_opened_session(['',''])\n",
    "        \n",
    "        print(\"EXITING...\")\n",
    "        return\n",
    "\n",
    "    ############################\n",
    "    \n",
    "        # if new session, then store in pickle \n",
    "    if(opened_url==''):\n",
    "        opened_url = driver.command_executor._url       #\"http://127.0.0.1:60622/hub\"\n",
    "        opened_session_id = driver.session_id            #'4e167f26-dc1d-4f51-a207-f761eaf73c31'        \n",
    "        \n",
    "        pickled_url_session_list=[]\n",
    "        pickled_url_session_list=[opened_url,opened_session_id]\n",
    "        save_opened_url_and_opened_session(pickled_url_session_list)\n",
    "        \n",
    "    time.sleep(2)\n",
    "\n",
    "    #Count total pages, by the bottom page navigation bar\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@class='artdeco-pagination__indicator artdeco-pagination__indicator--number ember-view']\")))\n",
    "    bottom_page_bar = driver.find_elements_by_xpath(\"//*[@class='artdeco-pagination__indicator artdeco-pagination__indicator--number ember-view']\")\n",
    "\n",
    "    # this gives the page number except of the current active page\n",
    "    last_page = int(bottom_page_bar[len(bottom_page_bar)-1].get_attribute(\"data-test-pagination-page-btn\"))\n",
    "\n",
    "    # So wait till the last page is not active\n",
    "    # current active page has class=\"artdeco-pagination__indicator artdeco-pagination__indicator--number active selected ember-view\"\n",
    "\n",
    "\n",
    "    while(job_page<=last_page):\n",
    "        try:\n",
    "            isStale = True\n",
    "            times=25 \n",
    "\n",
    "            # Fetching jobs at one page of LinkedIn\n",
    "            while(isStale==True or times>=0):\n",
    "                try:\n",
    "                    # Finding Compelete Side tab\n",
    "                    WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[starts-with(@class, 'jobs-search-results__list-item occludable-update p0')]\")))\n",
    "                    jobs_desc_small_tabs = driver.find_elements_by_xpath(\"//*[starts-with(@class, 'jobs-search-results__list-item occludable-update p0')]\")\n",
    "                    WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[starts-with(@class, 'jobs-search-results__list-item occludable-update p0')]\")))\n",
    "\n",
    "                    # Finding parent elements which are used to extracted job id from their child \n",
    "                    # inside of job_id_parent_elements LIST find immediate child of every ElEMENT \n",
    "                    # and that child has JOB ID as HREF attribute \n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@class='mr1 artdeco-entity-lockup__image artdeco-entity-lockup__image--type-square ember-view']\")))\n",
    "                    job_id_25_parent_elements = driver.find_elements_by_xpath(\"//*[@class='mr1 artdeco-entity-lockup__image artdeco-entity-lockup__image--type-square ember-view']\")\n",
    "                    job_id_25_parent_elements[len(job_id_25_parent_elements)-1].location_once_scrolled_into_view\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@class='mr1 artdeco-entity-lockup__image artdeco-entity-lockup__image--type-square ember-view']\")))\n",
    "\n",
    "\n",
    "                    print(\"Jobs small tab len and job_id_parent_length\")\n",
    "                    print(len(jobs_desc_small_tabs))\n",
    "                    print(len(job_id_25_parent_elements))\n",
    "\n",
    "                    isStale = False\n",
    "                    times = times-1;    \n",
    "\n",
    "                except StaleElementReferenceException as e:\n",
    "                    print('Still Stale... Update element')\n",
    "                    isStale = True\n",
    "                except Exception as e:\n",
    "                    print('Exception occured : '+str(e))\n",
    "                    isStale = True\n",
    "\n",
    "\n",
    "            # GETTING DETAILS about Job\n",
    "\n",
    "            # After fecthing the jobs in small tabs we need to fecth the BIG description and other details by clicking on it\n",
    "            # Check Job Id in the Saved_jobs\n",
    "\n",
    "            # First Check Job Id is new OR it is already saved on disk\n",
    "            job_id_link_list_25=[]\n",
    "\n",
    "            # Making list of job id links from elements\n",
    "            for jid_ele in job_id_25_parent_elements:                    \n",
    "                # jod id element is in immediate child\n",
    "                actual_jid_ele = jid_ele.find_element_by_xpath(\"*\")\n",
    "                jid_href = actual_jid_ele.get_attribute(\"href\")\n",
    "                job_id_link_list_25.append(jid_href)\n",
    "\n",
    "\n",
    "\n",
    "            job_tab_index=0;\n",
    "            \n",
    "            # Checking whether job is already stored\n",
    "            # for every job in side tab : getting JobId and CompanyId from the jobs on current page and CHECK its EXISTENCE\n",
    "            for jid_link in job_id_link_list_25:\n",
    "                \n",
    "                                \n",
    "                is_job_already_stored = False\n",
    "                trimmed_jid = trim_link_for_job_id(str(jid_link))\n",
    "\n",
    "                # Check job if already stored in disk or not\n",
    "                for job_detail in saved_jobs_list:\n",
    "                    if(trimmed_jid==job_detail[\"Job_id\"]):\n",
    "                        is_job_already_stored =True\n",
    "\n",
    "                # if job is new scrap it and store on the disk\n",
    "                if(is_job_already_stored==False):                \n",
    "                    print('####################################################################################')\n",
    "                    print('New job : '+str(i))\n",
    "\n",
    "                    # We're clicking 'ADDRESS' of job location in small tab desc for opening big decription \n",
    "                    try:\n",
    "                        # if job is not on disk parse details of that job\n",
    "                        safe_click_area = jobs_desc_small_tabs[job_tab_index].find_element_by_xpath(\".//*[@class='job-card-container__metadata-wrapper']\")\n",
    "                        safe_click_area.click()\n",
    "\n",
    "\n",
    "                        # Clicking on side tab gives us JobID and GeoId in the URL\n",
    "                        # extract NEW JOB id using window URL. The only way till now.\n",
    "                        time.sleep(0.5)\n",
    "                        job_view_url = driver.current_url\n",
    "\n",
    "                        extracted_jid = re.compile(r'currentJobId=\\d+').search(job_view_url).group(0)\n",
    "                        extracted_jid = extracted_jid.split('currentJobId=')[1]    \n",
    "                        \n",
    "                        extracted_gid = re.compile(r'geoId=\\d+').search(job_view_url).group(0)\n",
    "                        extracted_gid = extracted_gid.split('geoId=')[1]\n",
    "\n",
    "                        jid_arg = extracted_jid\n",
    "                        gid_arg = extracted_gid\n",
    "                        \n",
    "                        job_desc_small_arg = jobs_desc_small_tabs[job_tab_index].text\n",
    "\n",
    "                        # Big description area\n",
    "                        WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class='jobs-box__html-content jobs-description-content__text t-14 t-normal']\")))\n",
    "                        job_description = driver.find_element_by_xpath(\"//*[@class='jobs-box__html-content jobs-description-content__text t-14 t-normal']\")\n",
    "                        job_desc_big_arg = job_description.text\n",
    "\n",
    "                        \n",
    "                        WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class='jobs-unified-top-card__subtitle-secondary-grouping t-black--light']\")))\n",
    "                        date_ele = driver.find_element_by_xpath(\"//*[@class='jobs-unified-top-card__subtitle-secondary-grouping t-black--light']/span\")\n",
    "                        date_text = date_ele.text\n",
    "                        \n",
    "                        cid_arg = the_company_id\n",
    "\n",
    "                        try:\n",
    "                            # Clicking on the 'APPLY' button to get the job link\n",
    "                            WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class='jobs-apply-button artdeco-button artdeco-button--icon-right artdeco-button--3 artdeco-button--primary ember-view']\")))\n",
    "                            job_button = driver.find_element_by_xpath(\"//*[@class='jobs-apply-button artdeco-button artdeco-button--icon-right artdeco-button--3 artdeco-button--primary ember-view']\")\n",
    "                            job_button.click()\n",
    "\n",
    "                            try:                            \n",
    "                                WebDriverWait(driver, 10).until(EC.number_of_windows_to_be(2))\n",
    "                                # To get the 'APPLY' link from newly opened tab\n",
    "                                driver.switch_to.window(driver.window_handles[1])                            \n",
    "                                time.sleep(0.5)\n",
    "                                t=0\n",
    "                                while(driver.current_url=='about:blank' and t<15):    \n",
    "                                    time.sleep(0.25)\n",
    "                                    t=t+0.25;\n",
    "\n",
    "                                arg_job_url=driver.current_url                        \n",
    "\n",
    "                                # Closing the job apply tab afetr getting it's URL\n",
    "                                driver.execute_script(\"window.stop();\")\n",
    "                                driver.close()\n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                            except NoSuchWindowException as e:\n",
    "                                driver.switch_to.window(driver.window_handles[1])\n",
    "                                driver.execute_script(\"window.stop();\")\n",
    "                                driver.close()                         \n",
    "                                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                                arg_job_url='NA'\n",
    "\n",
    "                            if(len(driver.window_handles)>=2):\n",
    "                                driver.switch_to.window(driver.window_handles[1])\n",
    "                                driver.execute_script(\"window.stop();\")\n",
    "                                driver.close()\n",
    "                                #print(\"CLosed new apply window in Exception\")\n",
    "\n",
    "                            driver.switch_to.window(window_name=driver.window_handles[0])\n",
    "\n",
    "\n",
    "                            # Sending everything to function to store \n",
    "                            make_dictionary(jid_arg,gid_arg,job_desc_small_arg,job_desc_big_arg,arg_job_url,cid_arg,date_text)\n",
    "                            display_job(saved_jobs_list[-1])\n",
    "\n",
    "                        except TimeoutException as ex:\n",
    "\n",
    "                            # if Job URL not available \n",
    "                            arg_job_url = 'NA'\n",
    "                            make_dictionary(jid_arg,gid_arg,job_desc_small_arg,job_desc_big_arg,arg_job_url,cid_arg,date_text)\n",
    "                            display_job(saved_jobs_list[-1])\n",
    "\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(\"EXCEPTION OCCURED Clicking Area : \"+str(e))\n",
    "\n",
    "                            if(len(driver.window_handles)>=2):\n",
    "                                driver.switch_to.window(driver.window_handles[1])\n",
    "                                driver.execute_script(\"window.stop();\")\n",
    "                                driver.close()\n",
    "\n",
    "                            driver.switch_to.window(window_name=driver.window_handles[0])\n",
    "\n",
    "                            print(traceback.format_exc())\n",
    "\n",
    "\n",
    "                    except TimeoutException as ex:\n",
    "                        print(\"TIMEOUT EXCEPTION OCCURED Clicking Area : CANT FIND BIG DESCRIPTION BOX \"+str(ex))\n",
    "                        print(traceback.format_exc())\n",
    "                    except Exception as e:          \n",
    "                        print(traceback.format_exc())\n",
    "                        print(\"EXCEPTION OCCURED, in looping through 25 jobs : \"+str(e))\n",
    "                        continue\n",
    "                \n",
    "                \n",
    "                job_tab_index+=1\n",
    "\n",
    "                # end for loop for 25 side_job_elements\n",
    "\n",
    "\n",
    "            # One page jobs scrapped!!!\n",
    "            # Now GO to next page\n",
    "            isStale = True\n",
    "            times=25 \n",
    "\n",
    "            # Checking for staleness of Bottom page bar element\n",
    "            while(isStale==True or (times>=0 and isStale==True)):\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class='artdeco-pagination__pages artdeco-pagination__pages--number']\")))\n",
    "                    bottom_page_element = driver.find_element_by_xpath(\"//*[@class='artdeco-pagination__pages artdeco-pagination__pages--number']\")\n",
    "                    bottom_page_element.location_once_scrolled_into_view\n",
    "                    immediate_children = bottom_page_element.find_elements_by_xpath(\"*\")\n",
    "                    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class='artdeco-pagination__pages artdeco-pagination__pages--number']\")))\n",
    "\n",
    "                    isStale = False\n",
    "                    times = times-1;             \n",
    "\n",
    "                except StaleElementReferenceException as e:\n",
    "                    print('Still Stale... Update element')\n",
    "                    isStale = True\n",
    "                except Exception as e:\n",
    "                    print('Exception occured while checking for staleness  : '+str(e))\n",
    "                    isStale = True\n",
    "\n",
    "            # Calculating the next page\n",
    "            if(job_page!=last_page):\n",
    "\n",
    "                time.sleep(0.5)\n",
    "                pos=0;        \n",
    "                for child in immediate_children:\n",
    "                    if(child.get_attribute(\"data-test-pagination-page-btn\")==str(job_page)):\n",
    "                        break;\n",
    "                    pos=pos+1;\n",
    "\n",
    "                # Click on the page button which will take to next page\n",
    "                immediate_children[pos+1].find_element_by_xpath(\"*\").click()\n",
    "\n",
    "\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "            job_page = job_page+1\n",
    "        except Exception as e:\n",
    "            print(\"EXCEPTION OCCURED outer EXITING...!!! : \"+str(e))\n",
    "            print(traceback.format_exc()) \n",
    "            break\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium code ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving jobs list after scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5893 saved jobs loaded!!!\n",
      "['http://127.0.0.1:61317', '4608eb20d20101b0198c2db7260f25c2']\n",
      " url and previous session loaded!!!\n",
      "Using previous session.\n",
      "<class 'str'>\n",
      "Bot gearing up...\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "7\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "10\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "13\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "16\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "18\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "20\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "23\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "Jobs small tab len and job_id_parent_length\n",
      "25\n",
      "25\n",
      "####################################################################################\n",
      "New job : 0\n",
      "Successfully saved : 5894 jobs on disk!\n",
      "Citi | Experience( Years ) : 12.0 | LinkedIn Job Id : 2615735879 | Date Posted 2021-06-05\n",
      "Return to Work\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9887030560\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 1\n",
      "Successfully saved : 5895 jobs on disk!\n",
      "Citi | Experience( Years ) : 11.0 | LinkedIn Job Id : 2615732502 | Date Posted 2021-06-23\n",
      "Testing Intermediate Analyst C11- DIGITAL - PUNE\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9887085712\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 2\n",
      "Successfully saved : 5896 jobs on disk!\n",
      "Citi | Experience( Years ) : 7.0 | LinkedIn Job Id : 2613825628 | Date Posted 2021-06-22\n",
      "IT Quality Intermediate Analyst\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9828187472\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 3\n",
      "Successfully saved : 5897 jobs on disk!\n",
      "Citi | Experience( Years ) : 7.0 | LinkedIn Job Id : 2613829445 | Date Posted 2021-06-22\n",
      "IT Quality Intermediate Analyst\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9828186512\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 4\n",
      "Successfully saved : 5898 jobs on disk!\n",
      "Citi | Experience( Years ) : 2.0 | LinkedIn Job Id : 2615733199 | Date Posted 2021-06-23\n",
      "IT Quality Analyst 2 C10 - DABI - PUNE\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9886987232\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 5\n",
      "Successfully saved : 5899 jobs on disk!\n",
      "Citi | Experience( Years ) : 0 | LinkedIn Job Id : 2606118253 | Date Posted 2021-06-20\n",
      "Backend Software Engineer\n",
      "Tel Aviv, Tel Aviv, Israel\n",
      "https://jobs.citi.com/job/-/-/287/9757220960\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 6\n",
      "Successfully saved : 5900 jobs on disk!\n",
      "Citi | Experience( Years ) : 8.0 | LinkedIn Job Id : 2615729741 | Date Posted 2021-06-23\n",
      "Digital (API) S/W Engineer/Analyst\n",
      "Pune, Maharashtra, India\n",
      "https://jobs.citi.com/job/-/-/287/9886946576\n",
      "\n",
      "\n",
      "####################################################################################\n",
      "New job : 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-747d2024535b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaved_jobs_list\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mload_saved_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscrap_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"11448\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-aa5513edd890>\u001b[0m in \u001b[0;36mscrap_jobs\u001b[1;34m(the_company_id, role)\u001b[0m\n\u001b[0;32m    287\u001b[0m                         \u001b[1;31m# Clicking on side tab gives us JobID and GeoId in the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                         \u001b[1;31m# extract NEW JOB id using window URL. The only way till now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m                         \u001b[0mjob_view_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saved_jobs_list =load_saved_jobs()\n",
    "scrap_jobs(\"11448\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
